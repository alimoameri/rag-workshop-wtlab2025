import streamlit as st
from ollama import chat
import numpy as np
from rich import print
import json
import pickle
import torch
from FlagEmbedding import BGEM3FlagModel
from tqdm.notebook import tqdm
from scipy import sparse
from sklearn.preprocessing import normalize
import pickle
import numpy as np


def lexical_to_csr(lexical_weights, vocab_size):
    """
    Convert list[defaultdict] â†’ csr_matrix of shape (num_docs, vocab_size)
    """
    data = []
    rows = []
    cols = []

    for i, word_dict in enumerate(lexical_weights):
        for token_id_str, weight in word_dict.items():
            token_id = int(token_id_str)
            data.append(float(weight))         # convert np.float16 â†’ float
            rows.append(i)
            cols.append(token_id)

    matrix = sparse.csr_matrix((data, (rows, cols)), shape=(len(lexical_weights), vocab_size))
    return matrix


def get_clean_chunk(chunk):
    items = []
    for doc_item in chunk.meta.doc_items:
        items.append({
            "label": doc_item.label,
            "page_number": doc_item.prov[0].page_no,
            "charspan": doc_item.prov[0].charspan
         })
                
    clean_chunk = {
        "meta": {"filename": chunk.meta.origin.filename, "headings": chunk.meta.headings},
        "text": chunk.text
    }
    if items:
        clean_chunk["meta"]["chunk_items"] = items

    return clean_chunk

def hybrid_search(query, top_k=5, alpha=0.5):
    """
    alpha = weight for dense similarity (0-1)
    """
    q = model.encode([query], return_dense=True, return_sparse=True)

    # ----- Dense -----
    q_dense = np.array(q["dense_vecs"], dtype=np.float32).reshape(1, -1)
    q_dense = normalize(q_dense, axis=1)
    dense_scores = (dense_vectors @ q_dense.T).squeeze()
    
    # ----- Sparse -----
    q_sparse = lexical_to_csr([q["lexical_weights"][0]], vocab_size)

    sparse_scores = (sparse_vectors @ q_sparse.T).toarray().squeeze()

    # ----- Hybrid Score -----
    scores = alpha * dense_scores + (1 - alpha) * sparse_scores

    # ----- Top-K -----
    top_idx = np.argsort(scores)[::-1][:top_k]
    return [(chunks[i], float(scores[i])) for i in top_idx]


def augment_prompt(query, related_chunks):
    context_texts = []
    for chunk, score in related_chunks:
        context_texts.append(str(get_clean_chunk(chunk)))

    context = "\n\n---\n\n".join(context_texts)

    prompt = f"""
    Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±:
    {query}

    ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ Ù¾Ø±Ø³Ø´â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡ Ù‡Ø³ØªÛŒ.
    ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡.

    Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø¯Ø± Ù…ØªÙ†â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø­ØªÙ…Ø§Ù‹ ÙÙ‚Ø· Ø¨Ù†ÙˆÛŒØ³:
    Â«Ù¾Ø§Ø³Ø®ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.Â»

    Ù‡ÛŒÚ† Ø¯Ø§Ù†Ø´ Ø¨ÛŒØ±ÙˆÙ†ÛŒØŒ Ø­Ø¯Ø³ÛŒ ÛŒØ§ Ø§Ø³ØªÙ†Ø¨Ø§Ø· ÙØ±Ø§ØªØ± Ø§Ø² Ù…ØªÙ† Ù…Ø¬Ø§Ø² Ù†ÛŒØ³Øª.

    Ø¯Ø± Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒØŒ Ù…Ù†Ø¨Ø¹ Ø±Ø§ Ø¨Ø§ Ø§ÛŒÙ† ÙØ±Ù…Øª Ø°Ú©Ø± Ú©Ù†:

    Ù†Ø§Ù… heading (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯)
    Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡ (page_number Ø§Ø² JSON)

    Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡ (Ù‚Ø§Ù„Ø¨ json):
    {context}
    """
    return prompt



# read from checkpoint
with open('checkpoints/embeddings-bge-m3.pkl', 'rb') as file:
    embeddings = pickle.load(file)

dense_vectors = np.array(embeddings["dense_vecs"], dtype=np.float32)
sparse_vectors = embeddings["lexical_weights"]  # scipy.sparse.csr_matrix

with open('checkpoints/clean_chunks.pkl', 'rb') as file:
    chunks = pickle.load(file)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using:", device)

model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, devices=device)
vocab_size = model.tokenizer.vocab_size
sparse_vectors = lexical_to_csr(embeddings["lexical_weights"], vocab_size)

# ------------------------
# STREAMLIT UI
# ------------------------

st.set_page_config(page_title="Persian RAG Assistant", layout="wide")

st.title("ğŸ” Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ")

# Query Input
query = st.text_input("Ù¾Ø±Ø³Ø´ Ú©Ø§Ø±Ø¨Ø±:", placeholder="Ù…Ø«Ø§Ù„: Ø´Ø±Ø§ÛŒØ· Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø­Ø±Ø§Ø² ØµÙ„Ø§Ø­ÛŒØª Ø§ÙØ±Ø§Ø¯ Ø§Ù…ØªÛŒØ§Ø²Ø¢ÙˆØ±")

top_k = st.slider("ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ Ù‚Ø·Ø¹Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ:", 1, 10, 5)
alpha = st.slider("âš–ï¸ ÙˆØ²Ù† Ø´Ø¨Ø§Ù‡Øª Ø¨Ø±Ø¯Ø§Ø±ÛŒ vs ÙˆØ§Ú˜Ú¯Ø§Ù†ÛŒ (alpha):", 0.0, 1.0, 0.7)

show_prompt = st.checkbox("ğŸ“„ Ù†Ù…Ø§ÛŒØ´ Ù¾Ø±Ø§Ù…Ù¾Øª Ø³Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡")

if st.button("ğŸš€ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ù¾Ø§Ø³Ø®"):
    if not query.strip():
        st.warning("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù¾Ø±Ø³Ø´ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
    else:
        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø³Ù†Ø§Ø¯..."):
            related_chunks = hybrid_search(query, top_k=top_k, alpha=alpha)

        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„..."):
            prompt = augment_prompt(query, related_chunks)
            response = chat(model="Gemma3:12b", messages=[{"role": "user", "content": prompt}])
            answer = response["message"]["content"]

        st.markdown("## âœ… Ù¾Ø§Ø³Ø®:")
        st.write(answer)

        if show_prompt:
            st.markdown("---")
            st.markdown("### ğŸ“„ Ù¾Ø±Ø§Ù…Ù¾Øª Ø³Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡")
            st.code(prompt, language="markdown")

        st.markdown("---")
        st.markdown("### ğŸ“š Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡")
        for i, (chunk, score) in enumerate(related_chunks, start=1):
            st.markdown(f"**[{i}] Ø§Ù…ØªÛŒØ§Ø²:** `{score:.4f}`")
            st.json(chunk)   # Assuming chunk is JSON-like
            st.markdown("---")

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fad657",
   "metadata": {},
   "source": [
    "# RAG Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3984c6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag-workflow.png\" width=\"60%\"></img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f1fd2",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08317962",
   "metadata": {},
   "source": [
    "+ Retreival is the core of a RAG system\n",
    "+ If retreival fails, whole system fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3541c5",
   "metadata": {},
   "source": [
    "## Diffrent Methods for IR\n",
    "\n",
    "| Method                            | Description                                    | Advantage                     | Disadvantage                   |\n",
    "| --------------------------------- | ---------------------------------------------- | ----------------------------- | ------------------------------ |\n",
    "| **Lexical/Keyword Retrieval** (e.g., TF-IDF) | Searches based on common words                 | Fast and simple               | Does not grasp deeper meaning  |\n",
    "| **Dense Retrieval** (Embedding)   | Searches based on **meaning** rather than words | More natural and accurate results | Requires an embedding model     |\n",
    "| **Hybrid Retrieval**              | Combination of both                            | Best quality                  | Slightly more complex          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99dc5a",
   "metadata": {},
   "source": [
    "### Keyword Search (Sparse Vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3f266b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/keyword-search.png\" width=\"70%\"></img>\n",
    "    <img src=\"images/bm25.jpeg\" width=\"40%\"></img>\n",
    "    <img src=\"images/bm25-explanation.png\" width=\"40%\"></img>\n",
    "</div>\n",
    "\n",
    "\n",
    "Sentence 1: The car is driven on the road.\n",
    "\n",
    "Sentence 2: The truck is driven on the highway.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/tf-idf.png\" width=\"50%\"></img>\n",
    "</div>\n",
    "\n",
    "\n",
    "```python\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "sentences_1 = [\"بهترین پیتزا فروشی\"]\n",
    "sentences_2 = [\"من فست فود دوست دارم.\", \"من پیتزا دوست دارم .\"]\n",
    "\n",
    "output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
    "output_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
    "\n",
    "\n",
    "# compute the scores via lexical mathcing\n",
    "lexical_scores = model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_2['lexical_weights'][0])\n",
    "print(lexical_scores)\n",
    "# 0\n",
    "\n",
    "lexical_scores = model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_2['lexical_weights'][1])\n",
    "print(lexical_scores)\n",
    "# 0.11194\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea98e53",
   "metadata": {},
   "source": [
    "### Semantic Search (Dense Vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62228995",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/semantic-search.png\" width=\"60%\"></img>\n",
    "</div>\n",
    "\n",
    "```python\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "sentences_1 = [\"بهترین پیتزا فروشی\"]\n",
    "sentences_2 = [\"من فست فود دوست دارم.\", \"من پیتزا دوست دارم .\"]\n",
    "\n",
    "embeddings_1 = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)\n",
    "# [[0.468  0.6655]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69a2ab",
   "metadata": {},
   "source": [
    "### Hybrid Search (sparse+dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bccdd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/hybrid-search.webp\" width=\"50%\"></img>\n",
    "</div>\n",
    "\n",
    "+ **Keyword Search**: Ensures sensitivity to exact words in user query.\n",
    "+ **Semantic Search**: Find documents with similar meaning, even without matching words.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/beta.png\" width=\"50%\"></img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fbc9c",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254bb74",
   "metadata": {},
   "source": [
    "+ Embeddings are numerical vector representations of text.\n",
    "+ They transform words, sentences, or documents into points in a high-dimensional space where semantic similarity corresponds to geometric closeness.\n",
    "+ If two texts mean similar things, their vectors will be close in the embedding space.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/embeddigns.png\" width=\"100%\"></img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc01d1b",
   "metadata": {},
   "source": [
    "# Evaluating a RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726b766",
   "metadata": {},
   "source": [
    "A RAG system has two parts that affect performance:\n",
    "\n",
    "| Component           | Question to Evaluate                           | Metric Type            |\n",
    "| ------------------- | ---------------------------------------------- | ---------------------- |\n",
    "| **Retriever**       | Does it find the *right documents*?            | Retrieval Metrics      |\n",
    "| **Generator (LLM)** | Does it produce a *correct and useful answer*? | Answer Quality Metrics |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda2f5a",
   "metadata": {},
   "source": [
    "##  Evaluating the Retriever\n",
    "\n",
    "We want to check whether the retriever returns relevant documents when given a query.\n",
    "\n",
    "### Key Retrieval Metrics\n",
    "\n",
    "| Metric          | What it Measures                                          | Interpretation             |\n",
    "| --------------- | --------------------------------------------------------- | -------------------------- |\n",
    "| **Recall@k**    | Whether the correct document appears in the top k results | Higher = Better retrieval  |\n",
    "| **Precision@k** | How many returned docs are actually relevant              | Higher = Cleaner retrieval |\n",
    "| **Hit Rate**    | Whether at least 1 relevant doc was retrieved             | Binary version of Recall@k |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a18e0",
   "metadata": {},
   "source": [
    "## Evaluating the Generated Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd73b451",
   "metadata": {},
   "source": [
    "Even with perfect retrieval, the LLM may:\n",
    "+ Hallucinate\n",
    "+ Ignore evidence\n",
    "+ Give incomplete answers\n",
    "\n",
    "So we evaluate the final generated output separately.\n",
    "\n",
    "### Answer Quality Metrics\n",
    "\n",
    "| Metric           | Meaning                                     | How to Score |\n",
    "| ---------------- | ------------------------------------------- | ------------ |\n",
    "| **Faithfulness** | Is the answer *grounded in retrieved text*? | 1–5 scale    |\n",
    "| **Correctness**  | Is the answer factually correct?            | 1–5 scale    |\n",
    "| **Completeness** | Is the answer sufficiently detailed?        | 1–5 scale    |\n",
    "| **Conciseness**  | Is the answer clear and not overly long?    | 1–5 scale    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e8645",
   "metadata": {},
   "source": [
    "### Automatic Evaluation (Model-as-a-Judge)\n",
    "\n",
    "```python\n",
    "evaluation_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "System Answer:\n",
    "{system_answer}\n",
    "\n",
    "Evaluate:\n",
    "1) Faithfulness: Does the answer rely only on context?\n",
    "2) Correctness: Is the answer accurate based on context?\n",
    "\n",
    "Return a score from 1 to 5 for each criterion.\n",
    "\"\"\"\n",
    "judge = chat(model=\"llama3:8b\", messages=[{\"role\": \"user\", \"content\": evaluation_prompt}])\n",
    "print(judge[\"message\"][\"content\"])\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chapter01-Intrudocion-to-LLMs-And-RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

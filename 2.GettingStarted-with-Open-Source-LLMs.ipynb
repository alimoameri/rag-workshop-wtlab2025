{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014df6b8",
   "metadata": {},
   "source": [
    "# Getting Started with Open-Source LLMs Using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ea18c",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) like LLaMA, Mistral, and Gemma are now available as open-source projects. You can easily run them **locally** using tools such as **Ollama**, **Hugging Face Transformers**, or **LM Studio** — all directly from Python.\n",
    "\n",
    "This guide explains how to start working with open-source LLMs using **Ollama** and **Hugging Face**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405d2cb",
   "metadata": {},
   "source": [
    "## 1. What Are Open-Source LLMs?\n",
    "\n",
    "Open-source LLMs are models whose **weights** and **inference code** are publicly available. Unlike proprietary models (e.g., GPT-4), you can:\n",
    "\n",
    "* Run them **offline**\n",
    "* Fine-tune them on your data\n",
    "* Embed them into your own apps or research projects\n",
    "\n",
    "Popular families include:\n",
    "\n",
    "* **LLaMA / Meta**\n",
    "* **Mistral / Mixtral**\n",
    "* **Gemma / Google**\n",
    "* **Falcon / TII**\n",
    "* **Qwen / Alibaba**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6b129",
   "metadata": {},
   "source": [
    "## 2. Choosing the Right Model\n",
    "\n",
    "+ **English**: https://www.vellum.ai/llm-leaderboard\n",
    "+ **Persian**: https://huggingface.co/spaces/MCINext/mizan-llm-leaderboard\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/mizan.png\" width=\"90%\"></img>\n",
    "</div>\n",
    "\n",
    "**Note**: Don't always trust leaderboards! Evaluate different models on your data. Each model's performance may vary for different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa02fa0",
   "metadata": {},
   "source": [
    "## 3. Running LLMs Locally with Ollama\n",
    "\n",
    "[Ollama](https://ollama.ai) provides a simple way to **download and run** open models locally — with GPU or CPU acceleration.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Linux / macOS\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Windows (PowerShell)\n",
    "winget install Ollama.Ollama\n",
    "```\n",
    "\n",
    "Verify installation:\n",
    "\n",
    "```bash\n",
    "ollama --version\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Running a Model from the CLI\n",
    "\n",
    "List available models:\n",
    "\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "\n",
    "Download and run a model (e.g., Gemma 2B):\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "ollama run gemma:2b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Using Ollama in Python\n",
    "\n",
    "Install the official Python client:\n",
    "\n",
    "```bash\n",
    "pip install ollama\n",
    "```\n",
    "\n",
    "Example code:\n",
    "\n",
    "```python\n",
    "from ollama import chat\n",
    "\n",
    "response = chat(model='gemma:2b', messages=[\n",
    "  {'role': 'system', 'content': 'You are a computer science expert.'},\n",
    "  {'role': 'user', 'content': 'Explain the difference between AI and machine learning.'}\n",
    "])\n",
    "\n",
    "print(response['message']['content'])\n",
    "```\n",
    "\n",
    "Ollama automatically caches models locally. You can integrate it into chat UIs, retrieval systems (RAG), or custom agents.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Using Hugging Face Transformers\n",
    "\n",
    "[Hugging Face Transformers](https://huggingface.co/transformers) is the most popular Python library for working with LLMs.\n",
    "It provides APIs to load, run, and fine-tune models from the **Hugging Face Hub**.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install transformers torch accelerate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Text Generation\n",
    "\n",
    "```python\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-270m-it\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who is the president of united states?\"},\n",
    "]\n",
    "pipe(messages)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Authentication and Model Access\n",
    "\n",
    "Some models (especially on Hugging Face) require authentication.\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(YOUR_HF_TOKEN)\n",
    "```\n",
    "\n",
    "For **Ollama**, no authentication is needed by default.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Other Useful Tools\n",
    "\n",
    "| Tool                                | Description                                      |\n",
    "| ----------------------------------- | ------------------------------------------------ |\n",
    "| **LM Studio**                       | GUI for running and testing open models          |\n",
    "| **vLLM**                            | High-performance inference engine                |\n",
    "| **LangChain**                       | Framework for building LLM apps                  |\n",
    "| **Llama.cpp**                       | C++ backend for running quantized models locally |\n",
    "\n",
    "---\n",
    "\n",
    "##  References\n",
    "\n",
    "* [Ollama Models](https://ollama.com//library)\n",
    "* [HuggingFace Pipelines](https://huggingface.co/docs/transformers/v4.57.1/en/main_classes/pipelines#)\n",
    "* [LangChain Docs](https://docs.langchain.com/oss/python/langchain/overview)\n",
    "\n",
    "## Appendix: Free API Providers for Quick Testing\n",
    "\n",
    "+ Openrouter\n",
    "+ ai.studio.google.com\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

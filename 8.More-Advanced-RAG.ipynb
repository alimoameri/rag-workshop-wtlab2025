{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0aae1d2",
   "metadata": {},
   "source": [
    "# Basic Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dcc994",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/basic-rag.svg\" width=\"50%\"></img>\n",
    "</div>\n",
    "\n",
    "+ This pattern is useful if the user asks for specific information about a topic that exists in one or more (but not too many) chunks. \n",
    "+ The question should not require complex aggregations or knowledge about the whole dataset. \n",
    "+ Since the pattern only contains a vector similarity search it is easy to understand, implement and get started with.\n",
    "+ Split documents into chunks and use an embedding model to embed the text content of the chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2822ed",
   "metadata": {},
   "source": [
    "#  Hypothetical Question Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b189ca",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/hypothetcal-question.svg\" width=\"50%\"></img>\n",
    "</div>\n",
    "\n",
    "+ The vector similarity between a question’s embedding and the text embedding of an appropriate answer or text source might be quite low. \n",
    "+  If we have question-chunk pairs available, we can execute a vector similarity search on the question embeddings, which will probably deliver much better results than a vector similarity search on the original text chunk.\n",
    "+ It requires more pre-processing effort and cost in LLM calls for the question generation.\n",
    "+ Use an LLM to generate hypothetical questions answered within the chunks. Embed the question using an embedding model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e15ee9",
   "metadata": {},
   "source": [
    "# Parent-Child Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d9c78",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/parent-child-retriever.svg\" width=\"50%\"></img>\n",
    "</div>\n",
    "\n",
    "+ Text embeddings represent a text’s semantic meaning.\n",
    "+ A more narrow piece of text will yield a more meaningful vector representation since there is less noise from multiple topics.\n",
    "+ However, if the LLM only receives a small piece of information for answer generation, the information might be missing context.\n",
    "+ Retrieving the broader surrounding text that the found information resides within solves the problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
